from typing import Dict, Any, List
from app.domain.entities.data_record import DataRecord # Still needed for return type hint, though list will be empty
# from app.domain.repositories.data_repository import IDataRepository # No longer needed
from app.domain.repositories.schema_repository import ISchemaRepository
from app.domain.exceptions import SchemaNotFoundException, InvalidDataException
from app.config.logging_config import logger
from app.infrastructure.persistence.high_performance_data_processor import HighPerformanceDataProcessor

import time

class CreateBulkDataRecordsUseCase:
    def __init__(self, high_performance_processor: HighPerformanceDataProcessor, schema_repository: ISchemaRepository):
        self.high_performance_processor = high_performance_processor
        self.schema_repository = schema_repository

    async def execute(self, schema_name: str, data_list: List[Dict[str, Any]]) -> List[DataRecord]:
        start_time = time.perf_counter()
        original_count = len(data_list)

        try:
            schema = await self.schema_repository.get_schema_by_name(schema_name)
            if not schema:
                raise SchemaNotFoundException(f"Schema '{schema_name}' not found")

            if not data_list:
                logger.info(f"Bulk insert for schema '{schema_name}' received an empty list. No action taken.")
                return []

            # Delegate to the high-performance processor
            # The processor handles Polars DataFrame creation, validation (type casting),
            # transformations (deduplication), and insertion.
            perf_metrics = await self.high_performance_processor.bulk_insert_ultra_fast(
                schema=schema,
                data=data_list
            )
            
            duration_ms = (time.perf_counter() - start_time) * 1000
            records_processed = perf_metrics.get("records_processed", 0) # Or derive from internal logic if needed
            
            # Note: The `bulk_insert_ultra_fast` method in HighPerformanceDataProcessor
            # already logs detailed performance. We can add a summary log here.
            logger.info(f"CreateBulkDataRecordsUseCase: Delegated bulk insert for {original_count} records to "
                       f"HighPerformanceDataProcessor. Processed: {records_processed}. Duration: {duration_ms:.2f}ms.")

            # The `bulk_insert_ultra_fast` method currently returns a Dict of metrics.
            # The use case is expected to return List[DataRecord].
            # This is a change in what can be returned. For now, let's return an empty list
            # as the actual DataRecord objects are not easily retrieved from the new path without significant changes.
            # Or, we can acknowledge this change in return type.
            # For now, let's return an empty list and log that DataRecord objects are not returned by this optimized path.
            # A more involved refactor could have bulk_insert_ultra_fast return the IDs or composite keys
            # of inserted records, which could then be fetched if needed, but that adds overhead.
            
            # TODO: Revisit the return type. The HighPerformance path doesn't easily yield DataRecord objects.
            # For now, we return an empty list and rely on logs for success/failure.
            # If DataRecord objects are strictly needed, the HPP would need to be modified to retrieve/construct them,
            # potentially impacting performance.
            logger.warning("CreateBulkDataRecordsUseCase with HighPerformanceDataProcessor does not currently return DataRecord objects.")
            
            # Placeholder: return empty list as DataRecord objects are not generated by HPPP.
            # This is a deviation from the original return type.
            # The original method constructed DataRecord objects. The new path does not.
            # This change needs to be acknowledged. For the purpose of this optimization,
            # we prioritize performance.
            return []

        except SchemaNotFoundException as e:
            logger.warning(f"Schema '{schema_name}' not found during bulk insert. Error: {str(e)}")
            raise
        except InvalidDataException as e: # Should be caught by HPP potentially
            logger.warning(f"Invalid data during bulk insert for schema '{schema_name}'. Error: {str(e)}")
            raise
        except Exception as e:
            duration_ms = (time.perf_counter() - start_time) * 1000
            logger.error(f"use_case_failed: CreateBulkDataRecordsUseCase, error={str(e)}, "
                           f"schema_name={schema_name}, records_count={original_count}, "
                           f"duration_ms={duration_ms:.2f}")
            raise 